CORE_NODES:

id: CN1
focus: "Базовая постановка задачи CIFAR-10 и текущая архитектура"
level: "машинное обучение"
role: "premise"
premises:

Используется CIFAR-10, разбиение на train/val/test и стандартная нормализация по каналам.

Конвейер данных настроен с аугментациями на обучении и «чистыми» данными на валидации/тесте.

Сеть — свёрточная, построенная на собственных слоях Conv/Pool/GAP, оптимизатор Adam, косинусный план обучения.

Текущие результаты: около 82 % точности при 20 эпохах, видимое плато и риск переобучения при изменении архитектуры.
local_conclusions:

Узкое место — не «сломанная» архитектура, а комбинация поля зрения, числа параметров, расписания скорости обучения и регуляризации.

Базовая схема уже разумна: улучшения нужно искать в планах обучения и аккуратных изменениях контекста (receptive field), а не в радикальном усложнении.
role_in_global_model: "Формирует исходное поле задачи и ограничения: цель 85 % на тесте без радикального усложнения сети."

id: CN2
focus: "Тройка архитектурных сценариев A/B/C для управления полем зрения"
level: "архитектура нейросетей"
role: "taxonomy"
premises:

Поле зрения можно увеличивать разными способами: крупными ядрами, факторизацией, дилатациями.

Для честного сравнения нужно фиксировать общий план обучения и регуляризацию.
local_conclusions:

Модель A: «большое сначала» — крупное ядро 5×5 в первом блоке, далее 3×3; ранний сбор контекста при умеренном числе параметров.

Модель B: факторизованное большое ядро (1×5 → 5×1) как экономный способ расширить поле зрения при меньшем числе параметров.

Модель C: дилатированные 3×3 во втором блоке для расширения поля зрения почти без роста числа параметров.
role_in_global_model: "Создаёт систематическое пространство архитектурных вариантов для A/B/C-сравнения влияния поля зрения."

id: CN3
focus: "Factorized convolution как механизм экономного расширения поля зрения"
level: "методы глубокого обучения"
role: "mechanism"
premises:

Обычное ядро 5×5 даёт большое поле зрения, но дорого по параметрам и FLOPs.

Две последовательные свёртки 1×5 и 5×1 охватывают примерно тот же участок входа.
local_conclusions:

Factorized convolution уменьшает число параметров и вычислений примерно в 2–2,5 раза при близком поле зрения.

Разделение ядра по осям повышает численную устойчивость по сравнению с чётными ядрами (4×4) и уменьшает склонность к переобучению.
role_in_global_model: "Обосновывает модель B как осмысленный кандидат: тот же контекст при меньшей сложности."

id: CN4
focus: "Цель пользователя и минималистичная лестница оптимизации до 85 %"
level: "методология эксперимента"
role: "interpretive_frame"
premises:

Пользователь не стремится к автоматическому перебору архитектур и «модным» техникам, а хочет понять поведение первой свёрточной сети.

Цель чётко задана: устойчиво выйти на ≈85 % точности на тесте, начиная с ≈82 %.
local_conclusions:

Первая ступень: скорректировать косинусный план обучения, введя нижний порог (eta_min) и увеличив число эпох до 40–50.

Вторая ступень: устранить нестабильные 4×4, заменив их на 3×3, не увеличивая сложность модели.

Третья ступень: при признаках переобучения слегка усилить L2-регуляризацию и при необходимости добавить один лёгкий Dropout после пулинга.
role_in_global_model: "Задаёт стратегию поэтапных, легко интерпретируемых изменений без скачка сложности."

id: CN5
focus: "Введение SGDR (cosine annealing with warm restarts) для выхода из плато"
level: "оптимизация обучения"
role: "mechanism"
premises:

Обычный косинусный спад шага к 10-й эпохе приводит к раннему «заглушению» градиента и плато.

SGDR реализует циклы: шаг падает по косинусу, затем резко возвращается к исходному значению (рестарт).
local_conclusions:

Циклическое восстановление скорости обучения позволяет покидать локальные минимумы, не меняя архитектуру.

Для CIFAR-10 разумны настройки: стартовый шаг 1e-3, минимальный шаг порядка 3e-5, циклы длиной около 10 эпох с последующим удвоением.
role_in_global_model: "Добавляет динамический инструмент борьбы с плато в рамках неизменной сети."

id: CN6
focus: "Ограниченный по амплитуде и частично рестартующий SGDR (sgdr_lr_frac)"
level: "тонкая настройка оптимизации"
role: "mechanism"
premises:

Полный рестарт LR «с 0 до 100 %» создаёт несколько эпох «восстановления», когда модель только возвращается к прежнему уровню.

Пользователь хочет, чтобы максимум после рестарта был ниже исходного (например, 70 % от lr0), а минимум не опускался слишком низко.
local_conclusions:

Вводятся параметры max_frac и min_frac, которые ограничивают диапазон LR внутри цикла; возможен дополнительный warm-up после рестарта.

Такая схема уменьшает амплитуду «пилы» валид.-метрик и ускоряет полезное обучение, поскольку градиент не уходит в слишком малые или слишком большие шаги.
role_in_global_model: "Уточняет базовый SGDR под конкретную кривую обучения, сводя к минимуму бесполезные фазы."

id: CN7
focus: "Манипулирование receptive field через число свёрток и способ даунсемплинга"
level: "архитектура нейросетей"
role: "mechanism"
premises:

Положение и способ даунсемплинга определяют, на каком разрешении сеть успевает собрать контекст.

Текущая архитектура довольно рано применяет MaxPool, сокращая пространство признаков.
local_conclusions:

Увеличение числа свёрток до пулинга (например, три слоя 5×5 → 3×3 → 3×3 до первого MaxPool) даёт более богатое поле зрения на полном разрешении.

Замена первого MaxPool на обучаемую свёртку 3×3 со stride=2 позволяет совместить сжатие с изучением контекста.

Конфигурации вида «3 свёртки в первом блоке, 2 во втором» часто дают лучший компромисс для CIFAR-10 без взрывного роста параметров.
role_in_global_model: "Определяет локальные архитектурные сдвиги, напрямую влияющие на пятно внимания и качество."

id: CN8
focus: "Кастомное расписание длины циклов SGDR (sgdr_lr_custom)"
level: "оптимизация обучения"
role: "mechanism"
premises:

Классический SGDR с T0 и T_mult задаёт жёстко возрастающие циклы (10, 20, 40, …), что не всегда совпадает с наблюдаемой динамикой ошибок.

Пользователю важны рестарты в конкретные моменты: например, 0, 10, 20, затем шаг в 20 эпох.
local_conclusions:

Вводится схема с явным списком длин циклов (например, 10, 10, 20), после чего повторяется последний интервал.

Это позволяет подстроить моменты рестартов под «рисунок» кривой валидации, сохранив идею косинусных дуг и частичных рестартов.
role_in_global_model: "Делает механизм SGDR согласованным с эмпирической динамикой конкретного эксперимента."

TRANSITIONS:

from: CN1
to: CN2
motive: "Переход от общей картины экспериментов к систематическому набору архитектурных вариантов для управления полем зрения."
gained_inference: "Понимание, что архитектурные изменения можно структурировать в три семейства (A/B/C), а не менять сеть хаотично."

from: CN2
to: CN3
motive: "Нужна детализация одного из вариантов — factorized conv — чтобы оценить его смысл, а не воспринимать как абстрактный трюк."
gained_inference: "Factorized conv объясняется как экономный способ сохранить поле зрения, тем самым оправдывая модель B как серьёзного претендента."

from: CN1
to: CN4
motive: "Уточнение цели пользователя и ограничений по сложности требует специальной экспериментальной стратегии."
gained_inference: "Формируется лестница небольших, последовательно применяемых улучшений, ориентированная на достижение 85 % без усложнения модели."

from: CN4
to: CN5
motive: "Наблюдаемое плато при косинусном спаде шага инициирует поиск более гибкого плана обучения."
gained_inference: "SGDR появляется как средство периодически восстанавливать скорость обучения и продолжать улучшение качества."

from: CN5
to: CN6
motive: "Практическое поведение SGDR (слишком резкие рестарты и долгие фазы восстановления) вызывает необходимость его смягчить."
gained_inference: "Вводятся частичные рестарты и ограниченный диапазон LR, что согласует SGDR с эмпирической кривой ошибок."

from: CN4
to: CN7
motive: "Помимо плана обучения, требуется осмысленно поправить архитектуру, не нарушая простоту сети."
gained_inference: "Вырабатываются локальные архитектурные приёмы: больше свёрток до даунсемплинга и обучаемое сжатие вместо «немого» MaxPool."

from: CN6
to: CN8
motive: "Выясняется, что не только амплитуда, но и временная структура циклов SGDR должна быть управляемой."
gained_inference: "Использование явного списка длин циклов позволяет согласовать моменты рестартов с наблюдаемой динамикой валидации."

from: CN7
to: CN4
motive: "Архитектурные изменения и новые планы LR должны быть вписаны обратно в поэтапную стратегию достижения 85 %."
gained_inference: "Лестница оптимизации обогащается архитектурными шагами и уточнёнными расписаниями LR, оставаясь при этом последовательной и интерпретируемой."
